{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D3TaLES/In-The-Mix/blob/main/data_science/InTheMix2_DataScineceDay2_MASTER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap\n",
        "1. Reduction and oxidation potentials are important for the development of redox flow batteries\n",
        "\n",
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/redox_flow_battery.png)\n",
        "\n",
        "2. There are several types of Machine Learning (ML).\n",
        "\n",
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/ml_taxanomy.png)\n",
        "\n",
        "\n",
        "Supervised learning is a form of machine learning involves those problems where the task at hand requires a fully labeled dataset. The models in this case use the labels as ground truth to learn and update themselves.Today we shall take a deeper dive in supervised learning and look at a practical application of it.\n"
      ],
      "metadata": {
        "id": "lrbnjTAfsDAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principles in Action: Training supervised ML models\n",
        ">Goal: Train ML model to predict vertical electron affinity.\n",
        "\n",
        "To accomplish this, we will download around 30,000 molecules and calculate three descriptors which we will use to predict the vertical electron affinity. We will start with simple models and then proceed to complex models.\n",
        "\n",
        "The goal is to see how well different models trained on our data performs the task of predicting vertical electron affinity.\n",
        "\n",
        "Before we proceed towards analysis let us spend some time discussing some terminologies that we will be using frequently in this tutorial and also learn about the important steps of approaching our task the ML way!"
      ],
      "metadata": {
        "id": "1OxBP6Dps0Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Terminology Explanation\n"
      ],
      "metadata": {
        "id": "iz3P4Ou22cS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Data visualization and preprocessing**\n",
        "\n",
        "> Data visualization helps us see patterns or trends. Data preprocessing typically involves cleaning up, organizing, and formatting the data so that it can be corretly handled by the model. This helps the model learn better from the data and make better predictions.\n"
      ],
      "metadata": {
        "id": "mA3KXgT3MF0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/train_test_split.png)\n",
        "> It is important to evaluate how our model performs on unseen data. To do so, people usually split the data into two parts â€“ one for teaching (training set) and one for checking (testing set). This helps us ensure that the machine learning model generalizes well on new, unseen information and not just memorizing the training data. The most commom way to do the train test split is randomly pick 80% of the data for training and the remainimg 20% for testing."
      ],
      "metadata": {
        "id": "266EFGRdMJt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/choose_the_right_model.png)\n",
        "\n",
        "> Just like different tools are used for different tasks, there are different machine learning models that are meant for different problems. Based on the type of problem and data we have, and the results we would want to achieve, the following are some tips for choosing a good model:\n",
        "1. Understand the problem: First, figure out what kind of problem you're trying to solve. Is it about classifying things (like cats and dogs), predicting numbers (like house prices), or finding relationships between data points (like how exercise affects health)?\n",
        "2. Explore the data: Take a look at your data to see what's in it. Are there images, text, or numbers? Are there missing values or unusual patterns? This can help you choose a model that works well with that kind of data.\n",
        "3. Start simple: Try starting with a simple model, like a decision tree. If it doesn't perform well, you can move on to more complex models.\n",
        "4. Experiment and compare: Test different models on your data and compare their performance. Keep track of their accuracy, speed, and any other factors that are important for your task.\n"
      ],
      "metadata": {
        "id": "KHjprLhAMQXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/loss.png)\n",
        "\n",
        ">A loss function is like a scorecard that tells us how good or bad the computer's predictions are compared to the real answers. Our goal is to help the computer get better at predicting by minimizing the difference between its predictions and the actual values. The loss function guides the computer to make adjustments and improve its performance. The following are two common loss functions:\n",
        "1. Mean Squared Error (MSE): This loss function calculates the average of the squared differences between the model's predictions and the answers. It's often used in problems where we want to predict numbers, like house prices or temperatures.\n",
        "2. Cross-Entropy: This loss function is used when we want to classify things, like deciding if a picture is of a cat or a dog. It measures how well our model can predict the correct category, rewarding the model if it's confident about the right answer and penalizing it if it's confident about the wrong one.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WjpY3xoFOFKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visual of loss function\n",
        "from IPython.display import Image\n",
        "Image(url='https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/Loss_Function.gif', width = 600)"
      ],
      "metadata": {
        "id": "JPHqaXkGsn_s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/hyperparameter.png)\n",
        "\n",
        "> When we build a machine learning model, there are a set of parameters whose optimum values are determined through the training of the model but still some parameters remain which are related to the architecture of the model and are not determined as a part of the training process. These parameters are known as hyperparameters. Hyperparameter tuning involves determination of the optimum value of these hyperparameters that best fits our model."
      ],
      "metadata": {
        "id": "BekqoDfeOOxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REST API class\n",
        "#@title Install and import required packages\n",
        "!pip install rdkit -qqq\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import json, warnings, requests, rdkit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem.rdMolDescriptors import *\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "USERNAME = 'd3tales.edu@gmail.com'\n",
        "PASSWORD = 'D3education'\n",
        "\n",
        "class RESTAPI(object):\n",
        "    def __init__(self, method=None, url=\"https://d3tales.as.uky.edu\", endpoint=None,\n",
        "                 login_endpoint='login', username=USERNAME, password=PASSWORD,\n",
        "                 upload_file=None, params=None, expected_endpoint=None, return_json=False):\n",
        "        \"\"\"\n",
        "        Upload a file to through d3tales.as.uky.edu file upload feature.\n",
        "\n",
        "        :param method: str, html method (such as post or get)\n",
        "        :param url: str, base url\n",
        "        :param endpoint: str, post or get endpoint url (not containing base url)\n",
        "        :param login_endpoint: str, login url (not containing base url)\n",
        "        :param username: str, user username\n",
        "        :param password: str, user password\n",
        "        :param upload_file: str, path to file to be uploaded\n",
        "        :param params: dict, form parameters for post\n",
        "        :param return_json: bool, get or post method returns json if true\n",
        "        \"\"\"\n",
        "        self.method = method\n",
        "        self.endpoint = \"{}/{}/\".format(url, endpoint).replace(\"//\", \"/\").replace(':/', '://')\n",
        "        self.login_endpoint = \"{}/{}/\".format(url, login_endpoint).replace(\"//\", \"/\").replace(':/', '://')\n",
        "        if expected_endpoint:\n",
        "            self.expected_endpoint = \"{}/{}/\".format(url, expected_endpoint).replace(\"//\", \"/\").replace(':/', '://')\n",
        "        self.user_data = dict(username=username, password=password) if username and password else None\n",
        "\n",
        "        self.client = self.get_client()\n",
        "        params.update(dict(csrfmiddlewaretoken=self.csrftoken, next='/')) if params else {}\n",
        "        self.params = params or {}\n",
        "        self.upload_file = upload_file\n",
        "        self.return_json = return_json\n",
        "\n",
        "        if self.method in [\"get\", \"GET\", \"Get\"]:\n",
        "            self.response = self.get_process()\n",
        "\n",
        "        elif self.method in [\"POST\", \"post\", \"Post\"]:\n",
        "            self.response = self.post_process()\n",
        "\n",
        "        if expected_endpoint:\n",
        "            if self.response.request.url != self.expected_endpoint:\n",
        "                warnings.warn(\"The {} response url for {} to {} did not match the expected response url\".format(\n",
        "                    self.upload_file, self.endpoint, self.method))\n",
        "\n",
        "    @property\n",
        "    def cookies(self):\n",
        "        return self.client.get(self.endpoint).cookies  # sets cookie\n",
        "\n",
        "    @property\n",
        "    def csrftoken(self):\n",
        "        # Retrieve the CSRF token for data post\n",
        "        return self.cookies['csrftoken'] if 'csrftoken' in self.cookies else self.cookies['csrf']\n",
        "\n",
        "    def get_client(self):\n",
        "        with requests.Session() as client:\n",
        "            if self.login_endpoint and self.user_data:\n",
        "                # Login\n",
        "                client.get(self.login_endpoint)  # sets cookie\n",
        "                csrftoken = client.cookies['csrftoken'] if 'csrftoken' in client.cookies else client.cookies['csrf']\n",
        "                self.user_data.update(dict(csrfmiddlewaretoken=csrftoken, next='/'))\n",
        "                # Submit login form\n",
        "                req = client.post(self.login_endpoint, data=self.user_data, headers=dict(Referer=self.login_endpoint))\n",
        "            return client\n",
        "\n",
        "    def post_process(self):\n",
        "        # Submit data form\n",
        "        file_data = dict(file=open(self.upload_file, 'rb')) if self.upload_file else None\n",
        "        req = self.client.post(self.endpoint, data=self.params, files=file_data,\n",
        "                               headers=dict(Referer=self.endpoint), cookies=self.cookies)\n",
        "        return_data = req.json() if self.return_json else req\n",
        "        return return_data\n",
        "\n",
        "    def get_process(self):\n",
        "        if self.params:\n",
        "            req = self.client.get(self.endpoint, data=self.params, headers=dict(Referer=self.endpoint), cookies=self.cookies)\n",
        "        else:\n",
        "            req = self.client.get(self.endpoint, headers=dict(Referer=self.endpoint))\n",
        "\n",
        "        return_data = req.json() if self.return_json else req\n",
        "        return return_data\n",
        "\n",
        "\n",
        "def get_prop(prop=\"reduction_potential\", limit=500):\n",
        "  query = \"mol_characterization.\" + prop + \"==true/mol_info.smiles=1&mol_characterization.\" + prop + \"=1/limit=\" + str(limit)\n",
        "  print(\"Collecting data through REST API...\")\n",
        "  response = RESTAPI(method='get', endpoint=\"restapi/molecules/\"+query,\n",
        "                      url=\"https://d3tales.as.uky.edu\", login_endpoint='login',\n",
        "                      return_json=True).response\n",
        "  comp_data = pd.DataFrame(response)\n",
        "  get_value = lambda c: c.get(prop).get(\"value\")\n",
        "  comp_data['smiles'] = comp_data.mol_info.apply(lambda c: c.get(\"smiles\"))\n",
        "  comp_data[prop] = comp_data.mol_characterization.apply(get_value)\n",
        "  comp_data\n",
        "\n",
        "  comp_data.set_index(comp_data._id, inplace=True)\n",
        "  comp_data.drop(['_id', 'mol_info', 'mol_characterization' ], axis=1, inplace=True)\n",
        "\n",
        "  return comp_data\n",
        "# get_prop(prop=\"vertical_electron_affinity\", limit=5)"
      ],
      "metadata": {
        "id": "BCfEd2EMtRCu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "\n",
        "Data preparation is an important part of machine learning. Data preparation involves getting the data ready to the point where it can reveal its secrets when we cast the spell of machine learning in it. Often the raw or original data that is available contains many irregularities which needs to be taken care of. In addition to that it is very important to gain a thorough understanding of every aspect of the data. This is necessary to understand and interpret results across the several steps of the analysis process. Even though the steps for prerpocessing can vary depending on the data we deal with the following steps broadly summarize how data can be prepared correctly:\n",
        "\n",
        "1. Gaining information about the source of the data.\n",
        "2. Doing a thorough study on the background of the data and what each aspect of the data signifies.\n",
        "3. Remove irregularities present in the data.\n",
        "4. Prepare proper visualization of the data.\n",
        "\n",
        "Let us now obtain our dataset and do a visual study of it to find out what we are dealing with.\n",
        "\n",
        "Hit run on the next cell. This will download the necessary data required for our supervised ml exercise!\n",
        "\n",
        "Our data for this consists four of properties of several molecules namely vertical electron affinity, kappa1, LabuteASA and CalcChi1v."
      ],
      "metadata": {
        "id": "bcAOYRknQJrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading and preprocessing data\n",
        "\n",
        "raw_df = get_prop(prop = 'vertical_electron_affinity', limit=100000).reset_index(drop=True)\n",
        "mol = []\n",
        "Kappa1 = []\n",
        "Kappa2 = []\n",
        "LabuteASA = []\n",
        "calc1v = []\n",
        "calc2v = []\n",
        "MolMR = []\n",
        "NumHD = []\n",
        "for i in range(len(raw_df)):\n",
        "        mol.append(Chem.MolFromSmiles(raw_df['smiles'][i]))\n",
        "        Kappa1.append(CalcKappa1(mol[i]))\n",
        "        LabuteASA.append(CalcLabuteASA(mol[i]))\n",
        "        calc1v.append(CalcChi1v(mol[i]))\n",
        "\n",
        "\n",
        "d = pd.DataFrame({'Kappa1':Kappa1, 'LabuteASA':LabuteASA, 'CalcChi1v':calc1v})\n",
        "d = d.join(raw_df['vertical_electron_affinity'])\n",
        "\n",
        "\n",
        "print(d)"
      ],
      "metadata": {
        "id": "C8gYXnHDsyaC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "Response = 'vertical_electron_affinity'\n",
        "# Response = 'oxidation_potential'\n",
        "\n",
        "Feature1 = 'Kappa1'\n",
        "Feature2 = 'CalcChi1v'\n",
        "Feature3 = 'LabuteASA'\n",
        "Sample_Size =  30814\n",
        "data_itm = pd.DataFrame({'Response':d[Response], 'Feature1':d[Feature1], 'Feature2':d[Feature2], 'Feature3':d[Feature3]}).sample(Sample_Size)\n",
        "#data_itm = pd.DataFrame({'Response':d[Response], 'Feature1':d[Feature1], 'Feature2':d[Feature2], 'Feature3':d[Feature3]})\n",
        "data_itm.head()"
      ],
      "metadata": {
        "id": "gEd04PDrPY7K",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now look at our data. We discussed earlier that this is an important part of data preparation. We human beings learn best using our basic senses. So its important to be able to see the data through our eyes.\n",
        "\n",
        "Hit run on the cell below!"
      ],
      "metadata": {
        "id": "WR1g5h0cnr4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing data\n",
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(data_itm, diag_kind='hist')"
      ],
      "metadata": {
        "id": "9esW-ux3lnmx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can immediately notice the weirdness present in the data just by looking at it. This happens because of the presence of influential observations called outliers. The reason outliers are called influential observations are simply because they can influence any machine learning models very drastically. Thus is is important to remove them before performing any kind of modelling.\n",
        "\n",
        "Click on the next cell to see how the data looks after removal of outliers!"
      ],
      "metadata": {
        "id": "GUSrDeDEG0Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Removing outliers\n",
        "data_itm = data_itm.loc[(data_itm['Response'] > -20) & (data_itm['Response'] < 20)]\n",
        "sns.pairplot(data_itm, diag_kind='hist')"
      ],
      "metadata": {
        "id": "IJZgFeo8EtoF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are visually satisfied, lets proceed to analyze it. But before we do so we need to split the data. For this exercise we will use 80% data for training our models and 20% for testing them.\n",
        "\n",
        "The next cell will perform this 80-20 split."
      ],
      "metadata": {
        "id": "EK11GmQZqewe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Splitting data for training and testing\n",
        "y = data_itm['Response']\n",
        "X = data_itm[['Feature1', 'Feature2', 'Feature3']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "wJo3l76_dFpG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression\n",
        "\n",
        "This is one of the very first statistical concepts that enabled performing supervised learning. Even though its remarkably simple it has proved useful in innumerable number of situations and also paved the way for development of more advanced models for supervised learning.\n",
        "\n",
        "Consider the data we saw yesterday in the primer on linear algebra section\n",
        "\n",
        "Student No. |Age | Height |\n",
        "----------- |----|--------|\n",
        "1           | 20 |  65.78|  \n",
        "2           | 22 |  71.52 |  \n",
        "3           | 21 |  69.40 |  \n",
        "4           | 21 |  68.22 |  \n",
        "5           | 23 |  67.79 |  \n",
        "\n",
        "Suppose we want to come up with a model which takes input the height of a student and predicts its age. The most simple relationship we can think about is\n",
        "\n",
        "$Age = Î± + Î² \\cdot Height$.\n",
        "\n",
        "Note that this relationship is linear hence the name linear regression. Now we have established a relationship but we have not quantified it yet in the sense that we don't know the values of $Î±$ and $\\beta$. How to find the right value of $Î±$ and $Î²$?\n",
        "\n",
        "\n",
        "It is evident that modeling can never give predictions with 100% accuracy. No matter how good of a model we fit there will always be present some error in our prediction. This being said we can frame our model as\n",
        "\n",
        "> $Age = Î± + Î² \\cdot Height + e$.\n",
        "\n",
        "Here we are trying to explain the age from height using the $Î±$ + $Î²$*Height part but still after that there remains a part that affects age which we doesn't know. We are denoting this part using $Ïµ$. Let us now write the equations for our 5 data points\n",
        "\n",
        "> $20 = Î± + Î² \\cdot 65.78 + e_1$\n",
        "\n",
        "> $22 = Î± + Î² \\cdot 71.52 + e_2$\n",
        "\n",
        "> $21 = Î± + Î² \\cdot 69.4 + e_3$\n",
        "\n",
        "> $21 = Î± + Î² \\cdot 68.22 + e_4$\n",
        "\n",
        "> $23 = Î± + Î² \\cdot 67.79 + e_5$\n",
        "\n",
        "Then this implies that\n",
        "\n",
        "> $e_1 = 20 - (Î± + Î² \\cdot 65.78)$.\n",
        "\n",
        "> $e_2 = 22 - (Î± + Î² \\cdot 71.52)$.\n",
        "\n",
        "> $e_3 = 21 - (Î± + Î² \\cdot 69.4)$.\n",
        "\n",
        "> $e_4 = 21 - (Î± + Î² \\cdot 68.22)$.\n",
        "\n",
        "> $e_5 = 23 - (Î± + Î² \\cdot 67.79)$.\n",
        "\n",
        "are our errors in prediction. We would want to choose a value of $Î±$ and $Î²$ that minimizes these errors as much as possible. This idea is achieved using the method of least squares. Let us define a function\n",
        "\n",
        "> $L(Î±, Î²) = \\frac{1}{5} \\sum\\limits_{i=1}^5 e_i^2 = \\frac{1}{5} \\sum\\limits_{i=1}^5 (Age_{\\ i} - (Î± + Î² \\cdot Height_{\\ i}))^2$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This is known as the the 'mean squared error'. The idea is to choose $Î±$ and $\\beta$ which can gives us the minimum value of $L(Î±, Î²)$. Such values of $Î±$ and $\\beta$ will give us our linear regression model.\n",
        "\n",
        "The mean squarred error is often minimized using an algorithm called gradient descent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IhKxqjNETMO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "Image(url='https://gbhat.com/assets/gifs/linear_regression.gif', width = 600)"
      ],
      "metadata": {
        "id": "32lq01Zg1e06",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now perform linear regression on our data on molecules. Hitting run on the next module will fit the linear regression to our molecules."
      ],
      "metadata": {
        "id": "dtZDXZwdObHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt5Uyf1Je14G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How has the linear regression performed?\n",
        "\n",
        "To test this we use our fitted regression model to make predictions on the test data which we then use to calculate a measure of accuracy known as coefficient of variation denoted by $R^2$.\n",
        "\n",
        "$R^2$ gives us a measure of how much the predicted and the actual values are alike with respect to their variability. Higher values of $R^2$ indicate better accuracy."
      ],
      "metadata": {
        "id": "3Kd-z7JKesbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "y_pred_reg = reg.predict(X_test)\n",
        "r2_score(y_test, y_pred_reg)"
      ],
      "metadata": {
        "id": "AASOA--qe0IC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks (NN) for Supervised Learning"
      ],
      "metadata": {
        "id": "kXvN36ZAss27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Networks is a computational learning system that translate a data input (given in a certain form) into a desired output (usually in a different form). For example we can built a neural network to classify images into different classes. In this case the input data is images (one form) and the output are real numbers (a form different from images).\n",
        "\n",
        "Neural networks were inspired by the human brain which is a complex network of neurons connected in a complicated way. Information passes from one neuron to another through the connections which enables us to perform our daily functions.\n",
        "\n",
        "Like human brain neural networks also contains neurons which are simple computational units arranged in a specific way (usually in several layers) and connected to each other.\n",
        "\n",
        "A picture of a simple neural network is shown below.\n",
        "\n",
        "![](https://raw.githubusercontent.com/D3TaLES/In-The-Mix/main/data_science/media/neural_net.png)\n",
        "\n",
        "The picture above represents a simple neural network called the feed forward neural network with 4 layers.\n",
        "\n",
        "The first layer is called the **input layer**. This is where the data enters the network. The number of neurons in this layer is equal to the number of input features (or input variables).\n",
        "\n",
        "The layers in the middle are called the **hidden layers**. It is here that most of the magic happens. As the data passes through these layers certain computations take place which finally lead us to the **output layer** which gives us the desired output. Note that there can be several neurons in output layers depending on the problem at hand.\n",
        "\n",
        "The neurons between any two layers are connected through weights. Weights are parameters in the network that enables transformation of the input data within the hidden layers.\n",
        "\n",
        "Suppose we have a neural network with 1 input layer, 3 hidden layers and 1 output layer. Let $W_1, W_2, W_3$ denote three matrices which contain the weights of the 3 layers. Let us denote the input to neural network using $x$\n",
        "\n",
        "Upon passing through $h_1$ a matrix multiplication between $x$ and $W_1$ is performed followed by passing through a function $f$ known as the activation function. This can be denoted as\n",
        "\n",
        "> $h_1 = f (W_1x)$\n",
        "\n",
        "Now $h_1$ passes thorugh the second layer which performs a matrix multiplication between $h_1$ and $W_2$ followed by passage through activation function. This can be again denoted by\n",
        "\n",
        "> $h_2 = f (W_2 h_1)$\n",
        "\n",
        "Follwing this pattern until the final hidden layer we have.\n",
        "\n",
        "> $h_3 = f (W_3 h_2)$.\n",
        "\n",
        "$h_3$ is finally passed through the output layer which gives us the predicted value $\\hat{y}$ for input $x$.\n",
        "\n",
        "> $\\hat{y} = f (W_o h_3)$.\n",
        "\n",
        "Then if $y$ is the actual value for $\\hat{y}$ (the ground truth) then $e = y - \\hat{y}$ is the error in predicting $y$. Thus if we have multiple observations $x_1, x_2,\\ldots,x_n$ passing through the neural network then for each $x_i$ we will have a prediction $\\hat{y_i}$ for. This means for every $x_i$ we will have errors\n",
        "\n",
        "> $e_i = y_i - \\hat{y_i}$.\n",
        "\n",
        "Using these $n$ $e_i$'s we can define a 'mean squared error' in a exactly similar way we defined for linear regression which is\n",
        "\n",
        "> $L= \\frac{1}{n} \\sum\\limits_{i=1}^n e_i^2$.\n",
        "\n",
        "This function gives us a sense of the loss we make during preeictions hence will be used as our loss function.\n",
        "\n",
        "Training of the neural network involves selecting a value of weights which minimizes this loss function. This is achieved using an algorithm called bakcpropagation. The gif below shows the working principle behind backpropagation.\n",
        "\n",
        "You can read more about it in [here](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1K4sbt3cRCxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "Image(url='https://miro.medium.com/v2/1*mTTmfdMcFlPtyu8__vRHOQ.gif', width = 600)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6E0qGeY3fXBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now build a neural network to predict electron affinity using the other two properties and see if we can achieve any increase in accuracy or not."
      ],
      "metadata": {
        "id": "x75haiYqgIv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating neural net\n",
        "Number_of_input_units =  3\n",
        "Number_of_output_units = 1\n",
        "Number_of_hidden_layers = 5#@param {type:\"integer\"}\n",
        "Number_of_hidden_units = []\n",
        "for i in range(0, Number_of_hidden_layers):\n",
        "            print(\"The number of neurons for hidden layer \" + str(i+1) + \":\")\n",
        "            ele = int(input())\n",
        "            Number_of_hidden_units.append(ele) # adding the element\n",
        "#Number_of_hidden_units - inp_list.split(\",\")\n",
        "Number_of_ephocs =  50#@param {type:\"integer\"}\n",
        "Batch_size =  128\n",
        "\n",
        "\n",
        "model_parameters = {}\n",
        "model_parameters['hidden_units'] = Number_of_hidden_units\n",
        "model_parameters['input_size'] = Number_of_input_units\n",
        "model_parameters['output_size'] = Number_of_output_units\n",
        "\n",
        "training_parameters = {}\n",
        "training_parameters['epoch'] = Number_of_ephocs\n",
        "training_parameters['batch_size'] = Batch_size\n",
        "training_parameters['loss'] = 'mean_squared_error'\n",
        "training_parameters['optimizer'] = 'adam'\n",
        "\n",
        "Target = ['Response']\n",
        "Feature = ['Feature1', 'Feature2','Feature3']\n",
        "\n",
        "X = data_itm[Feature].values\n",
        "y = data_itm[Target].values\n",
        "\n",
        "FeatureScaler=StandardScaler()\n",
        "TargetVarScaler=StandardScaler()\n",
        "\n",
        "# Storing the fit object for later reference\n",
        "FeatureScalerFit=FeatureScaler.fit(X)\n",
        "TargetVarScalerFit=TargetVarScaler.fit(y)\n",
        "\n",
        "# Generating the standardized values of X and y\n",
        "X=FeatureScalerFit.transform(X)\n",
        "y=TargetVarScalerFit.transform(y)\n",
        "\n",
        "  # Split the data into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "d59b_oPPPoTc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to train the network\n",
        "def train_and_test_model(dataset, Target, Feature, model_params, train_params, test, verbose=True):\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  X = dataset[Feature].values\n",
        "  y = dataset[Target].values\n",
        "\n",
        "  FeatureScaler=StandardScaler()\n",
        "  TargetVarScaler=StandardScaler()\n",
        "\n",
        "  # Storing the fit object for later reference\n",
        "  FeatureScalerFit=FeatureScaler.fit(X)\n",
        "  TargetVarScalerFit=TargetVarScaler.fit(y)\n",
        "\n",
        "  # Generating the standardized values of X and y\n",
        "  X=FeatureScalerFit.transform(X)\n",
        "  y=TargetVarScalerFit.transform(y)\n",
        "\n",
        "  # Split the data into training and testing set\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test, random_state=42)\n",
        "\n",
        "  # start creating the model\n",
        "\n",
        "  #from keras.models import Sequential\n",
        "  #from keras.layers import Dense\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # Defining the Input layer and FIRST hidden layer, both are same!\n",
        "  model.add(Dense(units=model_params['hidden_units'][0], input_dim=model_params['input_size'], kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "  # add layers as given by the user\n",
        "  for layer_num, units_per_layer in enumerate(model_params['hidden_units']):\n",
        "    if layer_num > 0:\n",
        "      # we already added the first hidden layer along with the input layer above\n",
        "      model.add(Dense(units=units_per_layer, kernel_initializer='normal'))\n",
        "\n",
        "  # add output layer\n",
        "  model.add(Dense(model_params['output_size'], kernel_initializer='normal'))\n",
        "\n",
        "  # Compiling the model\n",
        "  model.compile(loss=train_params['loss'], optimizer=train_params['optimizer'])\n",
        "\n",
        "  # Fitting the ANN to the Training set\n",
        "  history = model.fit(X_train, y_train ,batch_size = train_params['batch_size'], epochs = train_params['epoch'], verbose=verbose)\n",
        "  losses = history.history['loss']\n",
        "  # calculate prediction and mean absolute relative error:\n",
        "  #MAPE = np.nanmean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
        "  MAPE = mean_absolute_percentage_error(y_test, model.predict(X_test))\n",
        "\n",
        "  return model, MAPE, losses\n"
      ],
      "metadata": {
        "id": "mZdQakO2Pvci",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training neural net\n",
        "model, err, losses = train_and_test_model(data_itm, Target, Feature, model_parameters, training_parameters, 0.2)\n",
        "\n",
        "epochs = range(1, len(losses) + 1)\n",
        "\n",
        "# Plot the losses\n",
        "plt.plot(epochs, losses, 'b', label='Training Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hLK_JcQMPz6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if the neural network has increased the accuracy or not!"
      ],
      "metadata": {
        "id": "bSOBLraLmhQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test, model.predict(X_test))\n"
      ],
      "metadata": {
        "id": "b5GmZISHSs_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not see a significant increase in accuracy! This is largely because the relationship between the variables and vertical electron affinity are far too complex to be captured by even this deep network.\n",
        "\n",
        "It might be possible if we increase the number of layers or neurons drastically we might see some decent results but that might lead to a non parsimonious model with unfeasible number of parameters.\n",
        "\n",
        "At this stage thus it is needed to look at more advanced networks which can capture this complex relationship present in this data."
      ],
      "metadata": {
        "id": "Nj-WsUq1mqaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to improve these models?\n",
        "\n",
        "All of the above models we developed until now have not given us an accuracy we would like to have. Thus the question arises how can we get a model which will increase the accuracy of prediction significantly. Some common ways include\n",
        "\n",
        "\n",
        "  - Increse data size.\n",
        "  - Larger model.\n",
        "  - Better features.\n",
        "\n",
        "Let us look at a tool which can perform predictions with a very good accuracy. Click on the link below\n",
        "\n",
        "[OCELOT ML](https://oscar.as.uky.edu/ocelotml_2d/)"
      ],
      "metadata": {
        "id": "ehH9iB3S4J4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synopsis\n",
        "\n",
        "When approaching any problem using machine learning it is necessary to\n",
        "\n",
        ">Perform a thorough visualization of the data to detect any influential observations.\n",
        "\n",
        "> Split the data into two parts for training and testing.\n",
        "\n",
        "> Define a proper loss function to ensure the chosen algorithm trains well.\n",
        "\n",
        "Often one simple model won't provide a good accuracy hence it is important to try out several models ranging from simple to complex models to find out the best model suited for the job at hand."
      ],
      "metadata": {
        "id": "xxtgBS4P4Vqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2021-2023, University of Kentucky and Iowa State University\n",
        "Designed by Souradeep Chattopadhyay, Chih-Hsuan Yang and Hsin-Jung Yang"
      ],
      "metadata": {
        "id": "xbPhtKP-GP5B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVziomh9PBiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}